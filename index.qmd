---
title: "Spark Streaming & Flink"
bibliography: bib.yaml
csl: iso690-author-date-de.csl
subtitle: ""
lang: de-DE
suppress-bibliography: true
format:
  revealjs:
    theme: [default, "patmanteau.scss"]
    slide-number: true
    incremental: true
    progress: true
    navigation-mode: linear
---

##

::: {.columns}

:::: {.column width="50%"}

Frameworks zur **verteilten Verarbeitung** großer Datenmengen

[Geistige Nachfolger von **Hadoop**, nur schneller und besser]{.fragment}

[Freie Software, Apache Software Foundation]{.fragment}

[**JVM**/Python]{.fragment}

::::


:::: {.column}

![](assets/spark-logo-hd.png)

![](assets/flink-header-logo.svg){width="70%"}

::::

:::

:::: {.notes}
Will nicht zu weit vorgreifen -- Spark ist Vorlesungsthema -- aber ich dippe mal den dicken Zeh in die Sparktasse

Frameworks zur verteilten Verarbeitung großer Mengen von Daten

Beide ähnliches Prinzip wie Hadoop, aber verzichten auf das Serialisieren von Zwischenergebnissen - daher häufig schneller
::::

## 

::: {.columns}

:::: {.column width="50%"}
![](assets/streaming-arch.png)
::::

:::: {.column width="50%"}

![](assets/flink-home-graphic.png)

::::
:::

**Ursprungsdaten** aus unterschiedlichen Quellen in **abgeleitete Daten** überführen^[Suchindizes, Empfehlungen, aggregierte Metriken, materialisierte Views uvvm.]

[durch Transformationen, Joins, Filterung, Aggregation, ML-Modelle, uvvm.]{.fragment}

[auf **einzelnen Maschinen** oder auf **Clustern** (mehr oder weniger agnostisch)]{.fragment}


:::: {.notes}

::::

## 

::: {.r-fit-text}
**Datenintegration**:

Daten in der

_richtigen Form_ an die

_richtigen Orte_ bringen.
:::

:::: {.notes}
Im ganzen Zusammenhang: Datenintegration. Definition bei @Kleppmann_DesigningDataIntensive_2017.
::::

## Dataflow Computing

Daten fließen durch mehrere Verarbeitungsstufen

```{.python code-line-numbers="1-2|3-4|5-6|7-8|1-8"}
# read data from text file
json_file = sc.textFile(r"ytmetadata.noid.json")
# split each line into words
words = json_file.flatMap(lambda line: line.split(" "))
# count the occurrence of each word
counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)
# save the counts to output
counts.saveAsTextFile("wordcount")
```


::: {.fragment}
Operationen als Knoten und Datenabhängigkeiten als Kanten eines Graphen

```{mermaid .fragment}
%%{init: {'theme': 'default', 'themeVariables': { 'fontFamily': 'Ubuntu Mono'}}}%%
graph LR;
    id1[textFile] --> id2("flatMap(split)") --> id4("map(tuple)") --> id5("reduceByKey(+)") --> id6[saveAsTextFile];
```
:::

##

Änderung in Quelldaten → Änderung in abgeleiteten Daten

Zwei Fragen:

- Wie schnell soll die Änderung "ankommen"?
- Kenne ich vorab die Größe der Ursprungsdaten?

##  

::::{.columns}
:::{.column}

### Batch Processing

**Beschränkte** Datenmengen, **bekannte, endliche** Größe

Neue abgeleitete Daten können aus historischen Daten erneut erzeugt werden

:::
:::{.column .fragment}

### Stream Processing

**Unbeschränkte** Datenmengen, **inkrementell** eintreffend

Abgeleitete Ansichten reagieren mit geringer Verzögerung auf Veränderungen im Eingang

:::
::::

::: {.notes}
@Kleppmann_DesigningDataIntensive_2017, Kap. 10, 11, 12
- Fraud detection systems need to determine if the usage patterns of a credit card have unexpectedly changed, and block the card if it is likely to have been stolen.
- Trading systems need to examine price changes in a financial market and execute trades according to specified rules.
- Manufacturing systems need to monitor the status of machines in a factory, and quickly identify the problem if there is a malfunction.
- Military and intelligence systems need to track the activities of a potential aggressor, and raise the alarm if there are signs of an attack.
:::

## Spark Streaming

![](assets/streaming-flow.png){width="60%"}

- Microbatching -- "Streams sind viele kleine Batches"
- Jeder Microbatch kann nach Ausfall neu gestartet werden
- Kompromiss: Overhead vs. Latenz


::::: {.notes}
Microbatching: Typische Batchgröße ist ca. 1 Sekunde. 
Kürzere Batchzeit - mehr Scheduling und Koordination
Längere Batchzeit - höhere Latenzzeit (HFT: wenn ich die Rechner schon möglichst nah an die Börse stelle,
will ich evtl. nicht unbedingt auf Spark warten müssen)
:::::

## Flink

- Streamframework -- aber jeder Batch ist streambar
- Fehlertoleranz via Checkpointing: Regelmäßige Barrieren im Stream, ab denen nach Ausfall wieder aufgesetzt werden kann
- Die Quelle muss das unterstützen (bspw. Kafka)

